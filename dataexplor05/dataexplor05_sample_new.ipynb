{
 "metadata": {
  "name": "",
  "signature": "sha256:ddcee7e51549352d22bc8d414bcac6c589f6ea70f2caab6d106a2b32dd537172"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(url=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.Image at 0x7f9cd4095690>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('https://raw.githubusercontent.com/TeachingDataScience/data-science-course/forstudentviewing/12_Naive_Bayes/twitter_training/sts_gold_tweet.csv',';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import numpy as np\n",
      "from itertools import product\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "models = [MultinomialNB(), BernoulliNB(), LogisticRegression()]\n",
      "num_words = [int(x) for x in list(np.linspace(20,1000,20))]\n",
      "vectorizers = []\n",
      "for counter in range(1,10):\n",
      "    vectorizers += [CountVectorizer(ngram_range=(1,counter), stop_words='english', max_features=587) for words in num_words]\n",
      "\n",
      "results ={}\n",
      "for model, vectorizer in product(models, vectorizers):\n",
      "    modelname = \"%s%d\"% (model.__class__.__name__, vectorizer.ngram_range[1])\n",
      "    kf = KFold(df.shape[0], 10)\n",
      "    cv_accuracies = []\n",
      "    for train, test in kf:\n",
      "        training_bag = vectorizer.fit_transform(df['tweet'][train])\n",
      "        testing_bag = vectorizer.transform(df['tweet'][test])\n",
      "        model.fit(training_bag, df['polarity'][train])\n",
      "        cv_accuracy = accuracy_score(model.predict(testing_bag), df['polarity'][test])\n",
      "        cv_accuracies.append(cv_accuracy)\n",
      "    results[modelname]={\n",
      "        'model': model.__class__.__name__,\n",
      "        'num_words': vectorizer.max_features,\n",
      "        'cv_accuracies': cv_accuracies,\n",
      "        'avg_accuracy': sum(cv_accuracies)/5,\n",
      "        'ngram_range' : vectorizer.ngram_range\n",
      "    }\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_dict(results.values()).sort(['avg_accuracy'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>avg_accuracy</th>\n",
        "      <th>cv_accuracies</th>\n",
        "      <th>model</th>\n",
        "      <th>ngram_range</th>\n",
        "      <th>num_words</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 0.817615</td>\n",
        "      <td> [0.791154791155, 0.746928746929, 0.83538083538...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 1</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0.812205</td>\n",
        "      <td> [0.788697788698, 0.742014742015, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 2</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 0.812204</td>\n",
        "      <td> [0.786240786241, 0.744471744472, 0.83046683046...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 3</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 0.811716</td>\n",
        "      <td> [0.788697788698, 0.737100737101, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 4</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 0.811710</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 5</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 0.811224</td>\n",
        "      <td> [0.781326781327, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 6</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 0.808284</td>\n",
        "      <td> [0.769041769042, 0.714987714988, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 1</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0.807780</td>\n",
        "      <td> [0.783783783784, 0.742014742015, 0.81326781326...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 7</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 0.807287</td>\n",
        "      <td> [0.781326781327, 0.744471744472, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 8</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0.806799</td>\n",
        "      <td> [0.776412776413, 0.742014742015, 0.80835380835...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 9</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 0.802875</td>\n",
        "      <td> [0.759213759214, 0.717444717445, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 7</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 0.802380</td>\n",
        "      <td> [0.756756756757, 0.717444717445, 0.82309582309...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 2</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 0.801892</td>\n",
        "      <td> [0.761670761671, 0.70515970516, 0.820638820639...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 3</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 0.801398</td>\n",
        "      <td> [0.759213759214, 0.710073710074, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 5</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.749385749386, 0.719901719902, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 6</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.761670761671, 0.707616707617, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 4</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 0.799434</td>\n",
        "      <td> [0.746928746929, 0.717444717445, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 8</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 0.797959</td>\n",
        "      <td> [0.756756756757, 0.700245700246, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 9</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 0.795990</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 1</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0.795010</td>\n",
        "      <td> [0.746928746929, 0.714987714988, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 4</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 0.795005</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81818181818...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 7</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0.795004</td>\n",
        "      <td> [0.744471744472, 0.714987714988, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 8</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0.794517</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 9</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 0.794023</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 3</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 0.793533</td>\n",
        "      <td> [0.739557739558, 0.710073710074, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 2</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 0.793038</td>\n",
        "      <td> [0.744471744472, 0.710073710074, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 5</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0.792550</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81081081081...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 6</td>\n",
        "      <td> 587</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "    avg_accuracy                                      cv_accuracies  \\\n",
        "13      0.817615  [0.791154791155, 0.746928746929, 0.83538083538...   \n",
        "12      0.812205  [0.788697788698, 0.742014742015, 0.82800982801...   \n",
        "11      0.812204  [0.786240786241, 0.744471744472, 0.83046683046...   \n",
        "17      0.811716  [0.788697788698, 0.737100737101, 0.82555282555...   \n",
        "16      0.811710  [0.786240786241, 0.749385749386, 0.82063882063...   \n",
        "15      0.811224  [0.781326781327, 0.749385749386, 0.82063882063...   \n",
        "22      0.808284  [0.769041769042, 0.714987714988, 0.83046683046...   \n",
        "14      0.807780  [0.783783783784, 0.742014742015, 0.81326781326...   \n",
        "10      0.807287  [0.781326781327, 0.744471744472, 0.82063882063...   \n",
        "9       0.806799  [0.776412776413, 0.742014742015, 0.80835380835...   \n",
        "21      0.802875  [0.759213759214, 0.717444717445, 0.81572481572...   \n",
        "23      0.802380  [0.756756756757, 0.717444717445, 0.82309582309...   \n",
        "24      0.801892  [0.761670761671, 0.70515970516, 0.820638820639...   \n",
        "19      0.801398  [0.759213759214, 0.710073710074, 0.81818181818...   \n",
        "20      0.800908  [0.749385749386, 0.719901719902, 0.81572481572...   \n",
        "18      0.800908  [0.761670761671, 0.707616707617, 0.82063882063...   \n",
        "25      0.799434  [0.746928746929, 0.717444717445, 0.81326781326...   \n",
        "26      0.797959  [0.756756756757, 0.700245700246, 0.81326781326...   \n",
        "0       0.795990  [0.751842751843, 0.707616707617, 0.82555282555...   \n",
        "3       0.795010  [0.746928746929, 0.714987714988, 0.81326781326...   \n",
        "6       0.795005  [0.744471744472, 0.717444717445, 0.81818181818...   \n",
        "7       0.795004  [0.744471744472, 0.714987714988, 0.82063882063...   \n",
        "8       0.794517  [0.739557739558, 0.712530712531, 0.81326781326...   \n",
        "2       0.794023  [0.744471744472, 0.717444717445, 0.81326781326...   \n",
        "1       0.793533  [0.739557739558, 0.710073710074, 0.82063882063...   \n",
        "4       0.793038  [0.744471744472, 0.710073710074, 0.81572481572...   \n",
        "5       0.792550  [0.739557739558, 0.712530712531, 0.81081081081...   \n",
        "\n",
        "                 model  ngram_range  num_words  \n",
        "13       MultinomialNB            1        587  \n",
        "12       MultinomialNB            2        587  \n",
        "11       MultinomialNB            3        587  \n",
        "17       MultinomialNB            4        587  \n",
        "16       MultinomialNB            5        587  \n",
        "15       MultinomialNB            6        587  \n",
        "22         BernoulliNB            1        587  \n",
        "14       MultinomialNB            7        587  \n",
        "10       MultinomialNB            8        587  \n",
        "9        MultinomialNB            9        587  \n",
        "21         BernoulliNB            7        587  \n",
        "23         BernoulliNB            2        587  \n",
        "24         BernoulliNB            3        587  \n",
        "19         BernoulliNB            5        587  \n",
        "20         BernoulliNB            6        587  \n",
        "18         BernoulliNB            4        587  \n",
        "25         BernoulliNB            8        587  \n",
        "26         BernoulliNB            9        587  \n",
        "0   LogisticRegression            1        587  \n",
        "3   LogisticRegression            4        587  \n",
        "6   LogisticRegression            7        587  \n",
        "7   LogisticRegression            8        587  \n",
        "8   LogisticRegression            9        587  \n",
        "2   LogisticRegression            3        587  \n",
        "1   LogisticRegression            2        587  \n",
        "4   LogisticRegression            5        587  \n",
        "5   LogisticRegression            6        587  "
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['percent_capitalized'] = df.tweet.apply(lambda x: sum([float(x[i] == x.upper()[i]) for i in range(len(x))])/len(x))\n",
      "df['exclamation'] = df.tweet.apply(lambda x: 1 if \"!\" in x else 0)\n",
      "df['at'] = df.tweet.apply(lambda x: 1 if \"@\" in x else 0)\n",
      "df['link'] = df.tweet.apply(lambda x: 1 if \"http\" in x else 0)\n",
      "\n",
      "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', max_features=587)\n",
      "bag_of_words = vectorizer.fit_transform(df['tweet'])\n",
      "model1=MultinomialNB()\n",
      "model1.fit(bag_of_words, df.polarity)\n",
      "predict = model1.predict(bag_of_words)\n",
      "df['polarity_hat']=np.array(predict)\n",
      "modelcovar = ['polarity_hat','percent_capitalized','exclamation','at','link']\n",
      "model2 = LogisticRegression()\n",
      "model2.fit(df[modelcovar],df.polarity)\n",
      "predictions = model2.predict(df[modelcovar])\n",
      "print accuracy_score(predictions, df['polarity'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.891347099312\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>polarity</th>\n",
        "      <th>percent_capitalized</th>\n",
        "      <th>exclamation</th>\n",
        "      <th>at</th>\n",
        "      <th>link</th>\n",
        "      <th>polarity_hat</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 2.034000e+03</td>\n",
        "      <td> 2034.000000</td>\n",
        "      <td> 2034.000000</td>\n",
        "      <td> 2034.000000</td>\n",
        "      <td> 2034.000000</td>\n",
        "      <td> 2034.000000</td>\n",
        "      <td> 2034.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 1.897987e+09</td>\n",
        "      <td>    1.242871</td>\n",
        "      <td>    0.288622</td>\n",
        "      <td>    0.305310</td>\n",
        "      <td>    0.375615</td>\n",
        "      <td>    0.034415</td>\n",
        "      <td>    1.193707</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 2.858502e+08</td>\n",
        "      <td>    1.851605</td>\n",
        "      <td>    0.088802</td>\n",
        "      <td>    0.460652</td>\n",
        "      <td>    0.484400</td>\n",
        "      <td>    0.182337</td>\n",
        "      <td>    1.830721</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td> 1.467812e+09</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.111111</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td> 1.557154e+09</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.239130</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td> 1.978393e+09</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.272029</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 2.174644e+09</td>\n",
        "      <td>    4.000000</td>\n",
        "      <td>    0.317073</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    0.000000</td>\n",
        "      <td>    4.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 2.329028e+09</td>\n",
        "      <td>    4.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    1.000000</td>\n",
        "      <td>    4.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "                 id     polarity  percent_capitalized  exclamation  \\\n",
        "count  2.034000e+03  2034.000000          2034.000000  2034.000000   \n",
        "mean   1.897987e+09     1.242871             0.288622     0.305310   \n",
        "std    2.858502e+08     1.851605             0.088802     0.460652   \n",
        "min    1.467812e+09     0.000000             0.111111     0.000000   \n",
        "25%    1.557154e+09     0.000000             0.239130     0.000000   \n",
        "50%    1.978393e+09     0.000000             0.272029     0.000000   \n",
        "75%    2.174644e+09     4.000000             0.317073     1.000000   \n",
        "max    2.329028e+09     4.000000             1.000000     1.000000   \n",
        "\n",
        "                at         link  polarity_hat  \n",
        "count  2034.000000  2034.000000   2034.000000  \n",
        "mean      0.375615     0.034415      1.193707  \n",
        "std       0.484400     0.182337      1.830721  \n",
        "min       0.000000     0.000000      0.000000  \n",
        "25%       0.000000     0.000000      0.000000  \n",
        "50%       0.000000     0.000000      0.000000  \n",
        "75%       1.000000     0.000000      4.000000  \n",
        "max       1.000000     1.000000      4.000000  "
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "class ModelTester:\n",
      "    def __init__(self):\n",
      "        self.models = []\n",
      "        \n",
      "    def add_model(self, name, modeltype, pre_processing = \"x\", vectorizer_args = {}, model_args={}):\n",
      "        vectorizer = CountVectorizer(**vectorizer_args)\n",
      "        model = modeltype(**model_args)\n",
      "        post_processing = df.tweet.apply(lambda x: eval(pre_processing))\n",
      "        bag_of_words = vectorizer.fit_transform(post_processing)\n",
      "        model.fit(bag_of_words, df.polarity)\n",
      "        data = {\n",
      "            'name':name,\n",
      "            'vectorizer':vectorizer,\n",
      "            'pre_processing': pre_processing,\n",
      "            'model': model\n",
      "        }\n",
      "        self.models.append(data)\n",
      "        \n",
      "    def test_models(self, tweet):\n",
      "        for model in self.models:\n",
      "            process = lambda x: eval(model['pre_processing'])\n",
      "            processed_tweet = process(tweet)\n",
      "            bag_of_words = model['vectorizer'].transform([processed_tweet])\n",
      "            print \"%s: %s\" % (model['name'],model['model'].predict(bag_of_words))\n",
      "        \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester= ModelTester()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.add_model(\"Jarret\", MultinomialNB, vectorizer_args={'max_features':587})\n",
      "modeltester.add_model(\"In Class Demo\", LogisticRegression, vectorizer_args={'ngram_range':(1,2)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"MODEL TESTER IS AWESOME!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jarret: [4]\n",
        "In Class Demo: [4]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"UGHHHH IT DOESNT LIKE LASSO!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jarret: [0]\n",
        "In Class Demo: [0]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}